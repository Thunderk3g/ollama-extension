# Instructions for Fixing the AI Portfolio Client Code

Hello brother! We've fixed issues in our extension, but there are a few more client code changes needed to be compatible with different Ollama versions and solve the 403/404 errors.

## Important: Ollama Version Compatibility

Your errors suggest you might be using an older version of Ollama that doesn't support the `/api/chat` endpoint. Our updated extension will automatically detect this and use the appropriate endpoint, but your client code also needs to be updated.

## Changes Needed in client.ts

1. **Update the `generateChatCompletion` function to handle both API versions:**

```typescript
export async function generateChatCompletion(params: OllamaChatParams): Promise<string> {
  // First check if extension is available
  const isExtensionAvailable = typeof window !== 'undefined' && 
                               window.OllamaBridge && 
                               window.OllamaBridge.isAvailable;
  
  try {
    // Make sure model is always included in the request
    if (!params.model) {
      throw new Error("Missing required 'model' parameter");
    }
    
    if (isExtensionAvailable) {
      // Use the extension for all API calls when available
      console.log(`Using Ollama Bridge extension with model: ${params.model}`);
      
      try {
        // Try the /api/chat endpoint first (newer Ollama versions)
        const response = await fetch('/api/chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            ...params,
            stream: false
          }),
        });
        
        if (response.ok) {
          const data = await response.json();
          return data.message?.content || '';
        } else if (response.status === 404) {
          // Fall back to /api/generate for older Ollama versions
          console.log('Chat endpoint not available, falling back to generate endpoint');
          
          // Convert messages to a prompt for the generate API
          let prompt = '';
          if (params.messages && params.messages.length > 0) {
            for (const msg of params.messages) {
              if (msg.role === 'system') {
                prompt += `System: ${msg.content}\n\n`;
              } else if (msg.role === 'user') {
                prompt += `User: ${msg.content}\n\n`;
              } else if (msg.role === 'assistant') {
                prompt += `Assistant: ${msg.content}\n\n`;
              }
            }
            
            // Add final prompt if the last message is from user
            if (params.messages[params.messages.length-1].role === 'user') {
              prompt += 'Assistant: ';
            }
          }
          
          // Call the generate API
          const generateResponse = await fetch('/api/generate', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              model: params.model,
              prompt: prompt,
              options: params.options || { temperature: 0.7 },
              stream: false
            }),
          });
          
          if (!generateResponse.ok) {
            const errorText = await generateResponse.text();
            throw new Error(`Ollama API error: ${errorText || generateResponse.statusText}`);
          }
          
          const generateData = await generateResponse.json();
          return generateData.response || '';
        } else {
          const errorText = await response.text();
          throw new Error(`Ollama API error: ${errorText || response.statusText}`);
        }
      } catch (error) {
        if (error.message.includes('403')) {
          throw new Error(`CORS Error: Try restarting Ollama with CORS enabled: "OLLAMA_ORIGINS=* ollama serve"`);
        }
        throw error;
      }
    } else {
      // No extension - use direct connection to Ollama
      const endpoint = await discoverOllamaEndpoint();
      if (!endpoint) {
        return "Couldn't connect to Ollama. Please make sure it's running locally.";
      }
      
      try {
        // Try the /api/chat endpoint first (newer Ollama versions)
        const chatResponse = await fetch(`${endpoint}/api/chat`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            ...params,
            stream: false
          }),
        });
        
        if (chatResponse.ok) {
          const data = await chatResponse.json();
          return data.message?.content || '';
        } else if (chatResponse.status === 404) {
          // Fall back to /api/generate for older Ollama versions
          console.log('Chat endpoint not available, falling back to generate endpoint');
          
          // Convert messages to a prompt for the generate API
          let prompt = '';
          if (params.messages && params.messages.length > 0) {
            for (const msg of params.messages) {
              if (msg.role === 'system') {
                prompt += `System: ${msg.content}\n\n`;
              } else if (msg.role === 'user') {
                prompt += `User: ${msg.content}\n\n`;
              } else if (msg.role === 'assistant') {
                prompt += `Assistant: ${msg.content}\n\n`;
              }
            }
            
            // Add final prompt if the last message is from user
            if (params.messages[params.messages.length-1].role === 'user') {
              prompt += 'Assistant: ';
            }
          }
          
          // Call the generate API
          const generateResponse = await fetch(`${endpoint}/api/generate`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              model: params.model,
              prompt: prompt,
              options: params.options || { temperature: 0.7 },
              stream: false
            }),
          });
          
          if (!generateResponse.ok) {
            const errorText = await generateResponse.text();
            throw new Error(`Ollama API error: ${errorText || generateResponse.statusText}`);
          }
          
          const generateData = await generateResponse.json();
          return generateData.response || '';
        } else {
          const errorText = await chatResponse.text();
          throw new Error(`Ollama API error: ${errorText || chatResponse.statusText}`);
        }
      } catch (error) {
        console.error('Error connecting to Ollama:', error);
        throw error;
      }
    }
  } catch (error) {
    console.error('Error generating chat completion:', error);
    return `Error: ${error instanceof Error ? error.message : 'Unknown error'}`;
  }
}
```

2. **Update the `sendChatMessage` function to handle the Ollama API compatibility:**

```typescript
export async function sendChatMessage(
  messages: ChatMessage[], 
  model: string, 
  agentType: string
): Promise<string> {
  try {
    // Always include the model parameter
    if (!model) {
      throw new Error("Missing required 'model' parameter");
    }
    
    // Format the messages for Ollama's API format
    const formattedMessages = messages.map(m => ({
      role: m.role as 'user' | 'assistant' | 'system',
      content: m.content
    }));
    
    if (agentType === 'resume') {
      // For resume agent, use the resume endpoint
      try {
        const resumeResponse = await fetch('/api/resume', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            model: model,
            messages: formattedMessages
          })
        });
        
        if (!resumeResponse.ok) {
          const errorText = await resumeResponse.text();
          throw new Error(`Resume API error: ${errorText || resumeResponse.statusText}`);
        }
        
        const data = await resumeResponse.json();
        
        // Handle both chat and generate API response formats
        if (data.message?.content) {
          // Chat API format
          return data.message.content;
        } else if (data.response) {
          // Generate API format
          return data.response;
        } else {
          return data.toString();
        }
      } catch (error) {
        console.error('Error using resume endpoint:', error);
        throw error;
      }
    }
    
    // For other agent types, use the generateChatCompletion function
    const response = await generateChatCompletion({
      model,
      messages: formattedMessages,
      options: {
        temperature: 0.7
      }
    });
    
    return response;
  } catch (error) {
    console.error('Error sending chat message:', error);
    
    // Check if we're on HTTPS and might need the extension
    const isHttps = typeof window !== 'undefined' && window.location.protocol === 'https:';
    let extensionMessage = '';
    let corsMessage = '';
    
    if (isHttps) {
      extensionMessage = `\n4. If you're accessing this site via HTTPS, make sure the Ollama Bridge extension is installed and enabled`;
    }
    
    if (error.message.includes('403')) {
      corsMessage = `\n5. You may need to restart Ollama with CORS enabled: "OLLAMA_ORIGINS=* ollama serve"`;
    }
    
    return `Sorry, I couldn't connect to the Ollama server. Please make sure:
    
1. Ollama is installed on your system (https://ollama.com)
2. The Ollama service is running (run 'ollama serve' in a terminal)
3. You have the required models installed (run 'ollama pull ${model}')${extensionMessage}${corsMessage}

Error details: ${error instanceof Error ? error.message : 'Unknown error'}`;
  }
}
```

## Fixing the 403 Forbidden Error

If you're still getting 403 Forbidden errors, it's likely due to CORS restrictions. To fix this:

1. Stop your Ollama server
2. Restart it with CORS allowed: `OLLAMA_ORIGINS=* ollama serve`

On Windows, you can do this with:
```
set OLLAMA_ORIGINS=*
ollama serve
```

On macOS/Linux:
```
OLLAMA_ORIGINS=* ollama serve
```

## Fixing the 404 Not Found Error

The 404 error suggests your Ollama version may be using the older API format. Our changes above should handle both API versions automatically.

## Important Changes Summary

1. Added fallback to `/api/generate` when `/api/chat` isn't available
2. Improved error handling for CORS issues
3. Added compatibility with both old and new Ollama API formats
4. Added instructions for enabling CORS in Ollama

After implementing these changes, reload your extension by going to `chrome://extensions/` and clicking the refresh icon on the Ollama Bridge extension.

Let me know if you need any additional help! 